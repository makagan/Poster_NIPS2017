\documentclass[final,unknownkeysallowed]{beamer}
\usepackage{etex}

\usetheme{RJH}
\usepackage[utf8]{inputenc}
\usepackage[frenchb]{babel}
\usepackage[orientation=landscape,size=a0,scale=2.5]{beamerposter}
\usepackage[overlay]{textpos}
\usepackage{url}
\usepackage{auto-pst-pdf}
\usepackage{pst-plot}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage[labelformat=empty]{caption}
\usepackage{framed}
\usepackage{wrapfig}
\usepackage{tikz}
 \usetikzlibrary{arrows}

\setlength{\TPHorizModule}{\paperwidth}
\setlength{\TPVertModule}{\paperheight}

\newcommand{\qedwhite}{\hfill \ensuremath{\Box}}
\newcommand\sbullet[1][.5]{\mathbin{\vcenter{\hbox{\scalebox{#1}{$\bullet$}}}}}

\definecolor{lightgreen}{rgb}{0.0,0.8,0.0}
\definecolor{lightblue}{rgb}{0.3,0.8,1.0}
\definecolor{lightred}{rgb}{0.874,0.180,0.105}
\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{lightgray}{rgb}{0.8,0.8,0.8}
\definecolor{shadecolor}{rgb}{0.9,0.9,0.9}

\title{\bf Learning to Pivot with Adversarial Networks \textcolor{red}{\#105}}
\author{Gilles Louppe, Michael Kagan and Kyle Cranmer\\[1.5ex]
{\tiny
\includegraphics[scale=0.3]{images/mail.png}~\url{g.louppe@uliege.be},~\url{makagan@slac.stanford.edu},~\url{kyle.cranmer@nyu.edu} \\[1.5ex]
\includegraphics[scale=0.35]{images/github.png}~\url{https://github.com/glouppe/paper-learning-to-pivot/}
}}

\footer{}
\date{}

\makeatletter
\newcommand{\superimpose}[2]{%
  {\ooalign{$#1\@firstoftwo#2$\cr\hfil$#1\@secondoftwo#2$\hfil\cr}}}
\makeatother

\renewcommand*{\normalfont}{\relax}

\begin{document}
\begin{frame}{}


%% Column 1 ==================================================================

\begin{textblock}{0.32}(0.0,-0.33)

%% Abstract ------------------------------------------------------------------

\begin{block}{Abstract \phantom{p}}
Several techniques for domain adaptation have been proposed to account for
differences in the distribution of the data used for training and testing. The
majority of this work focuses on a binary domain label. Similar problems occur
in a scientific context where there may be a continuous family of plausible data
generation processes associated to the presence of systematic uncertainties.
Robust inference is possible if it is based on a {\color{blue}pivot -- a quantity whose
distribution does not depend on the unknown values of the nuisance parameters
that parametrize this family of data generation processes}. In this work,  we
introduce and derive theoretical results for {\color{red} a training procedure based on
adversarial networks for enforcing the pivotal property (or, equivalently,
fairness with respect to continuous attributes) on a predictive model.} The
method includes a hyperparameter to control the trade-off between accuracy and
robustness. We demonstrate the effectiveness of this approach with a toy example
and examples from particle physics.
\end{block}

\vspace{0.5cm}
\begin{block}{Problem statement \phantom{p}}

\begin{columns}[t]
\begin{column}{0.02\textwidth}
\end{column}
\begin{column}{0.48\textwidth}
Data generating process $p(X, Y, Z)$ \\
- $x \in \mathcal{X}$ are the data \\
- $y \in \mathcal{Y}$ are the target labels \\
- $z \in \mathcal{Z}$ are continuous or categorical \\
\hspace{1.2cm} domain labels, nuisance parameters or \\
\hspace{1.2cm} attribute we wish to be insensitive to.
\end{column}
%\begin{column}{0.02\textwidth}
%\end{column}
\begin{column}{0.48\textwidth}
\textbf{\color{red} Goal}: Learn a classification/regression function $f$
with parameters $\theta_{f}$ and minimizing a loss ${\cal L}_f(\theta_f)$ (e.g., the cross-entropy) such that
$$p(f(X ; \theta_f) = s | z ) = p(f(X ; \theta_f) = s | z^\prime ).$$
This implies that {\color{blue}\bf $f(X ; \theta_f)$ and $Z$ are independent variables}.
\end{column}
\begin{column}{0.02\textwidth}
\end{column}
\end{columns}

\end{block}

\vspace{0.5cm}
\begin{block}{Method \phantom{p}}

Let $r := p_{\theta_r}(z | f(X;\theta_f)=s)$ be {\color{red} an adversary network} modeling the conditional distribution $Z|f(X;\theta_f)$, with
parameters $\theta_r$ and loss ${\cal L}_r(\theta_f, \theta_r)$. As in GANs, $f$ and $r$ are trained simultaneously, by considering the value function
\begin{align}
E(\theta_f, \theta_r) = {\cal L}_f(\theta_f) - {\cal L}_r(\theta_f, \theta_r)
\end{align}
that we optimize by finding the minimax solution
\begin{align}
\smash{\hat\theta_f, \hat\theta_r} = \arg \min_{\theta_f} \max_{\theta_r} E(\theta_f, \theta_r).
\end{align}

\begin{center}
\tikzstyle{every node}=[font=\fontsize{25}{11.2}]
\begin{figure}

   \def\layersep{1cm}

   \begin{tikzpicture}[shorten >= 1pt, ->, node distance=\layersep,scale=1.7]
   \tikzstyle{neuron} = [circle, minimum size=0.25cm, draw=black!20, line width=0.3mm, fill=white]

   % Classifier f
   \node at (2,0) {Classifier $f$};
   \draw (-1,-0.5) rectangle (4,-5.5);

   \path[->, shorten >= 0pt] (-2,-3) edge (-1,-3);
   \node[left] at (-2,-3) {$X$};

   \path[-o, shorten >= 0pt] (1.5,-6.5) edge (1.5,-5.5);
   \node[below] at (1.5,-6.5) {$\theta_f$};

   \path[->, shorten >= 0pt] (3.5,-3) edge (6.5,-3);
   \node[above] at (5.25,-3) {$f(X;\theta_f)$};

   \path[dashed,-] (5.25,-3) edge (5.25,-6.5);
   \node[below] at (5.25,-6.5) {${\cal L}_f(\theta_f)$};

   \foreach \name / \y in {1,...,3}
       \node[neuron] (f-I-\name) at (-0.5,-1-\y) {};

   \foreach \name / \y in {1,...,5}
       \node[neuron] (f-H1-\name) at (-0.5cm+\layersep,-\y cm) {};
   \foreach \name / \y in {1,...,5}
       \node[neuron] (f-H2-\name) at (-0.5cm+3*\layersep,-\y cm) {};

   \node[neuron] (f-O) at (-0.5cm+4*\layersep,-3cm) {};

   \foreach \source in {1,...,3}
       \foreach \dest in {1,...,5}
           \path[black] (f-I-\source) edge (f-H1-\dest);

   \foreach \source in {1,...,5}
       \path[black] (f-H2-\source) edge (f-O);

   \node[black] at (1.5,-3) {...};

   % Adversary r
   \node at (11.75,0) {Adversary $r$};
   \draw (6.5,-0.5) rectangle (11.5,-5.5);

   \node[above] at (13.25,-2) {$\gamma_1(f(X;\theta_f);\theta_r)$};
   \path[-o, shorten >= 0pt] (11,-2.0) edge (15,-2.0);
   \node[above] at (13.25,-3) {$\gamma_2(f(X;\theta_f);\theta_r)$};
   \path[-o, shorten >= 0pt] (11,-3) edge (15,-3);
   \node[above] at (13.25,-4) {$\dots$};
   \path[-o, shorten >= 0pt] (11,-4) edge (15,-4);

   \path[-o, shorten >= 0pt] (9,-6.5) edge (9,-5.5);
   \node[below] at (9,-6.5) {$\theta_r$};

   \foreach \name / \y in {1,...,1}
       \node[neuron] (r-I-\name) at (7,-2-\y) {};

   \foreach \name / \y in {1,...,5}
       \node[neuron] (r-H1-\name) at (7cm+\layersep,-\y cm) {};
   \foreach \name / \y in {1,...,5}
       \node[neuron] (r-H2-\name) at (7cm+3*\layersep,-\y cm) {};

   \node[neuron] (r-O1) at (7cm+4*\layersep,-2cm) {};
   \node[neuron] (r-O2) at (7cm+4*\layersep,-3cm) {};
   \node[neuron] (r-O3) at (7cm+4*\layersep,-4cm) {};

   \foreach \source in {1,...,1}
       \foreach \dest in {1,...,5}
           \path[black] (r-I-\source) edge (r-H1-\dest);

   \foreach \source in {1,...,5}
       \path[black] (r-H2-\source) edge (r-O1);
   \foreach \source in {1,...,5}
       \path[black] (r-H2-\source) edge (r-O2);
   \foreach \source in {1,...,5}
       \path[black] (r-H2-\source) edge (r-O3);

   \node[black] at (9,-3) {...};

   \draw (15,-1.5) rectangle (18,-4.5);
   \path[->, shorten >= 0pt] (16.5,-0.5) edge (16.5,-1.5);
   \node[above] at (16.5,-0.5) {$Z$};
   \path[->, shorten >= 0pt] (16.5,-4.5) edge (16.5,-5.5);
   \node[below] at (16.5,-5.5) {$p_{\theta_r}(Z|f(X;\theta_f))$};
   \node at (16.5,-3) {${\cal P}(\gamma_1, \gamma_2, \dots)$};

   \draw[dashed,-] (16.5,-5) -| (12.75,-6.5);
   \node[below] at (12.75,-6.5) {${\cal L}_r(\theta_f, \theta_r)$};

   \end{tikzpicture}
\end{figure}

%}
\end{center}

{\bf Figure 1.} Architecture for the adversarial training of a binary classifier
$f$ against a nuisance parameters $Z$. The adversary $r$ models the
distribution $p(z|f(X;\theta_f)=s)$ of the nuisance parameters as observed only through the output $f(X;\theta_f)$ of the classifier. By
maximizing the antagonistic objective ${\cal L}_r(\theta_f, \theta_r)$, the classifier
$f$ forces $p(z|f(X;\theta_f)=s)$ towards the prior $p(z)$, which happens
when $f(X;\theta_f)$ is  independent of the nuisance parameter $Z$ and therefore pivotal.

\end{block}

\end{textblock}



%% Column 2 ==================================================================

\begin{textblock}{0.32}(0.331,-0.33)

\begin{block}{Theoretical motivation \phantom{p}}

Assume that ${\cal L}_f$ and ${\cal L}_r$  are respectively set to the
expected value of the
negative log-likelihood of $Y|X$ under $f$ and of $Z|f(X;\theta_f)$ under
$r$:
\begin{align}
    {\cal L}_f(\theta_f) &= \mathbb{E}_{x \sim X}  \mathbb{E}_{y \sim Y|x} [ -\log p_{\theta_f} (y|x) ]  \\
    {\cal L}_r(\theta_f, \theta_r) &= \mathbb{E}_{s \sim f(X;\theta_f)}  \mathbb{E}_{z \sim Z|s} [-\log p_{\theta_r} (z|s)]
\end{align}

\begin{shaded}
\textbf{Proposition 1.}
\textit{If there exists a minimax solution $(\smash{\hat\theta}_f, \smash{\hat\theta}_r)$
for Eqn.~(2) such that
$E(\hat\theta_f, \hat\theta_r) = H({Y|X}) - H(Z)$, then
$f(\cdot;\smash{\hat\theta}_f)$ is both an optimal classifier and a pivotal
quantity.}
\end{shaded}

Proof. (\textit{sketch})\\
\begin{enumerate}[(i)]
\item For fixed $\theta_f$, adversary $r$ is optimal at $\hat{\hat\theta}_r = \arg \max_{\theta_r} E(\theta_f, \theta_r)  = \arg \min_{\theta_r} {\cal L}_r(\theta_f, \theta_r)$,\\
in which case $p_{\hat{\hat\theta}_r}(z|f(X;\theta_f)=s) = p(z|f(X;\theta_f)=s)$ $\forall z, s$, and\\
${\cal L}_r \to \mathbb{E}_{s \sim f(X;\theta_f)} [ H({Z|f(X;\theta_f)=s}) ]$;

\item Value function $E$ can be restated as a function of $\theta_f$ only: \\
$E'(\theta_f) = {\cal L}_f(\theta_f) -  H({Z|f(X;\theta_f)})$;

\item We have the lower bound $H({Y|X}) - H(Z) \leq {\cal L}_f(\theta_f) - H({Z|f(X;\theta_f)})$ \\
where the equality holds at $\smash{\hat\theta}_f = \arg \min_{\theta_f} E'(\theta_f)$ when: \\
 $\sbullet$ $\smash{\hat\theta}_f$ minimizes the negative log-likelihood of $Y|X$ under $f$,
    which happens when $\smash{\hat\theta}_f$ are the parameters
    of an optimal classifier. Then, ${\cal L}_f$ reduces to
    minimum value $H({Y|X})$. \\
  $\sbullet$ $\smash{\hat\theta}_f$ maximizes the conditional entropy
    $H({Z|f(X;\theta_f)})$, since $H(Z|f(X;\theta)) \leq H(Z)$.
\end{enumerate}


$\Rightarrow$ By assumption, the bound is active, hence $H(Z|f(X;\theta_f)) = H(Z)$
because of the second condition, which happens when $Z$ and $f(X;\theta_f)$
are independent variables.
% In other words,  the
% optimal classifier $f(\cdot;\smash{\hat\theta}_f)$ is also a pivotal
% quantity.
\qedwhite

\end{block}

\vspace{0.2cm}
\begin{block}{In practice\phantom{p}}
The assumption of existence of an optimal and pivotal classifier may
not hold because $Z$ directly shapes the decision boundary.
% In this case, the lower bound is strict: $f$ can either be an optimal classifier or a
% pivotal quantity, but not both simultaneously.
However, the value function $E$ can be rewritten  as
\begin{align}
E_\lambda(\theta_f, \theta_r) = {\cal L}_f(\theta_f) - \lambda {\cal L}_r(\theta_f, \theta_r)
\end{align}
where $\lambda \geq 0$ is a hyper-parameter controlling the trade-off between
the performance of $f$ and its independence with respect to the nuisance
parameter. \\
- Setting $\lambda$ to a large value preferably enforces $f$ to be pivotal.\\
- Setting $\lambda$ close to $0$ constrains $f$ to be optimal.\\
The optimal choice of $\lambda$ should be \textcolor{red}{guided by a higher-level objective}.
\end{block}

\vspace{0.2cm}
\begin{block}{Toy example \phantom{p}}
\begin{figure}
    \begin{center}
        \includegraphics[width=0.245\textwidth]{figures/f-plain.pdf}
        \includegraphics[width=0.245\textwidth]{figures/surface-plain.pdf}
        \includegraphics[width=0.245\textwidth]{figures/f-adversary.pdf}
        \includegraphics[width=0.245\textwidth]{figures/surface-adversary.pdf}
    \end{center}
\end{figure}

{\bf Figure 2.}   (Left) Decision scores
       without adversarial training, showing clear dependence on $Z$
   (Middle left) Decision surface. Samples are easier to classify for values of $Z$ above $\sigma$.
   (Middle right) Decision scores with adversarial training.
      The resulting densities are now almost identical to each other, indicating only a
      residual dependency on $Z$.
   (Right) Adversarial
      training reshapes the decision function vertically to erase the dependency on $Z$.

\end{block}

\end{textblock}


\vspace{-0.3cm}

%% Column 3 ==================================================================

\begin{textblock}{0.32}(0.662,-0.33)

\begin{block}{Examples \phantom{p}}

\vspace{-0.8cm}
\begin{figure}
\centering
\begin{minipage}{.42\linewidth}
    \begin{center}
        \includegraphics[width=\textwidth]{figures/training.pdf}\vspace{-1em}
    \end{center}
    {\bf Figure 3.} Training curves for the toy example.
    Initialized with a pre-trained classifier $f$, adversarial training was for $200$ iterations, mini-batches of size $M=128$, $K=500$ and $\lambda=50$
\end{minipage}
\hspace{.05\linewidth}
\begin{minipage}{.42\linewidth}
    \begin{center}
        \includegraphics[width=\textwidth]{figures/ams.pdf}\vspace{-1em}
    \end{center}
        {\bf Figure 4.} Physics example. Approximate median significance as a function of the threshold
        on $f$. At $\lambda=10$, {\color{red} trading
        accuracy for independence
        results in a net benefit in terms of statistical significance}.
\end{minipage}
\end{figure}
\vspace{-0.5cm}
\begin{figure}
\centering
\begin{minipage}{.42\linewidth}
    \begin{center}
        \includegraphics[width=\textwidth]{figures/gender-without.pdf}
    \end{center}
\end{minipage}
\hspace{.05\linewidth}
\begin{minipage}{.42\linewidth}
    \begin{center}
        \includegraphics[width=\textwidth]{figures/gender-with.pdf}
    \end{center}
\end{minipage}
\end{figure}
{\bf Figure 5.} Building a fair classifier independent of gender. (Left) Without adversarial training, decision scores depend on gender. (Right) With adversarial training.

\end{block}

\vspace{0.5cm}
\begin{block}{Conclusions \phantom{p}}

\begin{itemize}
\item[{\color{green} \cmark}] Proposed a flexible learning procedure {\color{red}for building a
predictive model that is independent of continuous or categorical nuisance
parameters} by jointly training two neural networks in an adversarial fashion.

\item[{\color{green} \cmark}] Motivated the algorithm by showing
that the minimax solution corresponds to a predictive model
that is both optimal and pivotal (if that models exists) or for which one can
tune the trade-off between power and robustness.

\item[{\color{green} \cmark}] Can be used in any situation
where the training data may not be representative of the real data the
predictive model will be applied to in practice.

\item[{\color{green} \cmark}] In a scientific context, this enables one to train predictive models that are insensitive to systematic uncertainties parametrized by continuous or categorical nuisance parameters.

\item[{\color{green} \cmark}] The approach extends to cases where independence of the predictive model with respect to
observed random variables is desired, {\color{red}as in fairness for
classification}. E.g., this has been used in jet tagging (Shimmin et al, 2017) for decorrelating a model against a {\color{red} continuous} attribute.
\end{itemize}


\vspace{-0.7cm}
\line(1,0){525}\\
KC and GL are both supported through NSF ACI-1450310, additionally KC is supported through PHY-1505463 and PHY-1205376 and GL through the NRB Big Data Chair. MK is supported by the US Department of Energy (DOE) under grant DE-AC02-76SF00515 and by the SLAC Panofsky Fellowship.
\end{block}




\end{textblock}




\end{frame}
\end{document}
